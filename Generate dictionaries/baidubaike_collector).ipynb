{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59d129fc",
   "metadata": {},
   "source": [
    "# This is for collecting small corpus on Baidubaike "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ea708d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source code from ： https://github.com/newer-plus/baidubaike-corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5086266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07e7d7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['baidubaike', 'admin', 'local']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#connect to mongo service\n",
    "client = MongoClient('put your mongo link here')\n",
    "client.list_database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169dd0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import requests as rq\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "from urllib.parse import unquote, urlparse, urlunparse\n",
    "import pymongo\n",
    "from html import unescape\n",
    "from multiprocessing import Pool as ProcessPool\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from multiprocessing import Value\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient\n",
    "import defs\n",
    "\n",
    "\n",
    "client = MongoClient('your mongo link')\n",
    "client.list_database_names()\n",
    "#creat a database\n",
    "db = client['baidubaike']\n",
    "#create a collection\n",
    "tasks = db['tasks']\n",
    "items = db['items']\n",
    "#pymongo.MongoClient().drop_database('baidubaike')\n",
    "#tasks = pymongo.MongoClient().baidubaike.tasks  # same the task into database\n",
    "#items = pymongo.MongoClient().baidubaike.items  # save the result\n",
    "tasks.create_index([('url', 'hashed')])  # create a index to follow the process\n",
    "items.create_index([('url', 'hashed')])\n",
    "count = items.count()  # the numbers of pages that have already collected\n",
    "num = Value('L',count)\n",
    "if tasks.count() == 0:  # here i started from three website of entries (literature, movie and series)\n",
    "    url1 = 'http://baike.baidu.com/item/文学'\n",
    "    url2 = 'http://baike.baidu.com/item/电影'\n",
    "    url3 = 'http://baike.baidu.com/item/电视剧'\n",
    "    d1 = {'url': url1}\n",
    "    d2 = {'url': url2}\n",
    "    d3 = {'url': url3}\n",
    "#    if not items.find({'url': {'$in': [url1, url2, url3]}}):\n",
    "    tasks.insert_many([d1, d2, d3])\n",
    "\n",
    "url_split_re = re.compile('[&+]')\n",
    "\n",
    "\n",
    "def clean_url(url):\n",
    "    url = urlparse(url)\n",
    "    return url_split_re.split(urlunparse((url.scheme, url.netloc, url.path, '', '', '')))[0]\n",
    "\n",
    "\n",
    "#this can apply to the baidu head\n",
    "header = {\n",
    "    \"User-Agent\":\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "        'Chrome/81.0.4044.122 Safari/537.36'}\n",
    "\n",
    "\n",
    "\n",
    "def run():\n",
    "    def main():\n",
    "        while tasks.count() > 0:\n",
    "            url = tasks.find_one_and_delete({})['url']  # Get a url and delete it in the queue\n",
    "            try:\n",
    "                sess = rq.get(url, headers=header)\n",
    "            except:\n",
    "                continue\n",
    "            web = sess.content.decode('utf-8', 'ignore')\n",
    "            urls = re.findall('href=\"(/item/.*?)\"', web)  # Find all in site links\n",
    "            for u in urls:\n",
    "                try:\n",
    "                    u = unquote(str(u))\n",
    "                except:\n",
    "                    continue\n",
    "                u = 'http://baike.baidu.com' + u\n",
    "                u = clean_url(u)\n",
    "                if not items.find_one({'url': u}):  # Add links that have not been queued to the queue\n",
    "                    tasks.update({'url': u}, {'$set': {'url': u}}, upsert=True)\n",
    "            soup = BeautifulSoup(web)\n",
    "            text = soup.find_all('div', class_='para')\n",
    "            # To crawl the information we need, we need regular expression knowledge to write according to the web page source code\n",
    "            if text:\n",
    "                text = ' '.join(\n",
    "                    [re.sub('[ \\n\\r\\t\\u3000]+', ' ', re.sub('<.*?>|\\xa0', '', unescape(str(t))).strip()) for t in\n",
    "                     text])  # Do some simple treatment to the results of crawling\n",
    "                title = re.findall(u'<title>(.*?)_百度百科</title>', str(soup.title))[0]\n",
    "                items.update({'url': url}, {'$set': {'url': url, 'title': title, 'text': text}}, upsert=True)\n",
    "                num.value +=1\n",
    "                print('%s, 爬取《%s》，URL: %s, 已经爬取%s' % (datetime.datetime.now(), title, url, num.value))\n",
    "\n",
    "                \n",
    "    p = ThreadPool(4, defs.main)  # 多线程爬取，4是线程数\n",
    "    p.close()\n",
    "    p.join()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pool = ProcessPool(4, defs.run)  # 多进程爬取，4是线程数\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    time.sleep(30)\n",
    "    pool.terminate()\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
